{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from data import mnist\n",
    "from data import CustomTensorDataset\n",
    "from model import MyAwesomeModel\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyAwesomeModel()\n",
    "model = model.to(device)\n",
    "(X_train, y_train), (X_test, y_test) = mnist()\n",
    "\n",
    "train_dataset_normal = CustomTensorDataset(tensors=(X_train, y_train), transform=None)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=16)\n",
    "\n",
    "test_dataset_normal = CustomTensorDataset(tensors=(X_test, y_test), transform=None)\n",
    "testloader = torch.utils.data.DataLoader(train_dataset_normal, batch_size=16)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "plt.imshow(images[10].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.11419999599456787, running_loss 3596.9889104366302\n",
      "Accuracy: 0.11419999599456787, running_loss 3596.431107521057\n",
      "Accuracy: 0.11419999599456787, running_loss 3596.4422175884247\n",
      "Accuracy: 0.11419999599456787, running_loss 3596.444270133972\n",
      "Accuracy: 0.11419999599456787, running_loss 3596.444499731064\n",
      "Accuracy: 0.11419999599456787, running_loss 3596.444516658783\n",
      "Accuracy: 0.11419999599456787, running_loss 3596.444525718689\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19773/533005443.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlog_ps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_ps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlops_1/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlops_1/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Move to GPU for acceleration\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        ## TODO: Implement the validation pass and print out the validation accuracy\n",
    "        with torch.no_grad():   \n",
    "            # images, labels = next(iter(testloader))\n",
    "            images, labels = X_test.to(device), y_test.to(device)\n",
    "            ps = torch.exp(model(images))\n",
    "            acc = torch.eq(labels.argmax(1), ps.argmax(1)).sum() / labels.size(0)\n",
    "                \n",
    "        print(f'Accuracy: {acc.item()}, running_loss {running_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to calculate the percentage of correct predictions. `equals` has binary values, either 0 or 1. This means that if we just sum up all the values and divide by the number of values, we get the percentage of correct predictions. This is the same operation as taking the mean, so we can get the accuracy with a call to `torch.mean`. If only it was that simple. If you try `torch.mean(equals)`, you'll get an error\n",
    "\n",
    "```\n",
    "RuntimeError: mean is not implemented for type torch.ByteTensor\n",
    "```\n",
    "\n",
    "This happens because `equals` has type `torch.ByteTensor` but `torch.mean` isn't implemented for tensors with that type. So we'll need to convert `equals` to a float tensor. Note that when we take `torch.mean` it returns a scalar tensor, to get the actual value as a float we'll need to do `accuracy.item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'equals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8911/3983701611.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Accuracy: {accuracy.item()*100}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'equals' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "print(f'Accuracy: {accuracy.item()*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is untrained so it's making random guesses and we should see an accuracy around 10%. Now let's train our network and include our validation pass so we can measure how well the network is performing on the test set. Since we're not updating our parameters in the validation pass, we can speed up our code by turning off gradients using `torch.no_grad()`:\n",
    "\n",
    "```python\n",
    "# turn off gradients\n",
    "with torch.no_grad():\n",
    "    # validation pass here\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "```\n",
    "\n",
    ">**Exercise:** Implement the validation loop below and print out the total accuracy after the loop. You can largely copy and paste the code from above, but I suggest typing it in because writing it out yourself is essential for building the skill. In general you'll always learn more by typing it rather than copy-pasting. You should be able to get an accuracy above 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.8125%\n",
      "Accuracy: 89.0625%\n",
      "Accuracy: 92.1875%\n",
      "Accuracy: 89.0625%\n",
      "Accuracy: 89.0625%\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "model.to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Move to GPU for acceleration\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        ## TODO: Implement the validation pass and print out the validation accuracy\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                ps = torch.exp(model(images))\n",
    "\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "                break\n",
    "        print(f'Accuracy: {accuracy.item()*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "If we look at the training and validation losses as we train the network, we can see a phenomenon known as overfitting.\n",
    "\n",
    "<img src='assets/overfitting.png' width=450px>\n",
    "\n",
    "The network learns the training set better and better, resulting in lower training losses. However, it starts having problems generalizing to data outside the training set leading to the validation loss increasing. The ultimate goal of any deep learning model is to make predictions on new data, so we should strive to get the lowest validation loss possible. One option is to use the version of the model with the lowest validation loss, here the one around 8-10 training epochs. This strategy is called *early-stopping*. In practice, you'd save the model frequently as you're training then later choose the model with the lowest validation loss.\n",
    "\n",
    "The most common method to reduce overfitting (outside of early-stopping) is *dropout*, where we randomly drop input units. This forces the network to share information between weights, increasing it's ability to generalize to new data. Adding dropout in PyTorch is straightforward using the [`nn.Dropout`](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout) module.\n",
    "\n",
    "```python\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        \n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "During training we want to use dropout to prevent overfitting, but during inference we want to use the entire network. So, we need to turn off dropout during validation, testing, and whenever we're using the network to make predictions. To do this, you use `model.eval()`. This sets the model to evaluation mode where the dropout probability is 0. You can turn dropout back on by setting the model to train mode with `model.train()`. In general, the pattern for the validation loop will look like this, where you turn off gradients, set the model to evaluation mode, calculate the validation loss and metric, then set the model back to train mode.\n",
    "\n",
    "```python\n",
    "# turn off gradients\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # validation pass here\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "\n",
    "# set model back to train mode\n",
    "model.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Add dropout to your model and train it on Fashion-MNIST again. See if you can get a lower validation loss or higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 10]), torch.Size([64, 1, 28, 28]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: Define your model with dropout added\n",
    "class ClassifierDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        \n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1,16,5),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16,8,5),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(8,4,5),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(1024,10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "model.to(device)\n",
    "model(images).shape, images.shape\n",
    "# model = ClassifierDropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.1875%\n",
      "Accuracy: 90.625%\n"
     ]
    }
   ],
   "source": [
    "## TODO: Train your model with dropout, and monitor the training progress with the validation loss and accuracy\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 2\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Move to GPU for acceleration\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        ## TODO: Implement the validation pass and print out the validation accuracy\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                ps = torch.exp(model(images))\n",
    "\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "                break\n",
    "        print(f'Accuracy: {accuracy.item()*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that the model is trained, we can use it for inference. We've done this before, but now we need to remember to set the model in inference mode with `model.eval()`. You'll also want to turn off autograd with the `torch.no_grad()` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import helper module (should be in the repo)\n",
    "import helper\n",
    "import random\n",
    "cpu = torch.device(\"cpu\")\n",
    "# Test out your network!\n",
    "\n",
    "model.eval()\n",
    "model.cpu()\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[random.randint(1, 20)]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.view(1, *img.shape)\n",
    "\n",
    "# Calculate the class probabilities (softmax) for img\n",
    "with torch.no_grad():\n",
    "    output = model.forward(img)\n",
    "\n",
    "ps = torch.exp(output)\n",
    "ps.cpu()\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.view(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Up!\n",
    "\n",
    "In the next part, I'll show you how to save your trained models. In general, you won't want to train a model everytime you need it. Instead, you'll train once, save it, then load the model when you want to train more or use if for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
